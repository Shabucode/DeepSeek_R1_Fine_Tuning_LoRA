{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth #install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import the relevant packages","metadata":{}},{"cell_type":"code","source":"##modeule for finetuning\nfrom unsloth import FastLanguageModel \n#check if hardware supports bgloat16 precision\nfrom unsloth import is_bfloat16_supported\nimport torch #pytorch\nfrom trl import SFTTrainer #trainer for supervised finetuning\nimport wandb #weights and biases\n#training hyperparameters\nfrom transformers import TrainingArguments\n#loading finetuning dataset\nfrom datasets import load_dataset \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Access and login to Hugging face token","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient #import kaggle secrets\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wnb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset',\n    job_type=\"training\"\n    anonymous=\"allow\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the model and tokenizer  - unsloth version of DeepSeek-R1-Distill-Llama-8B.","metadata":{}},{"cell_type":"code","source":"max_seq_length = 2048\ndtype = None\nload_in_4bit = True\n#loading model in 4 bit quantization to optimise memory usage and performance\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Deepseek-R1-Distill-Llama-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model inference before finetuning\n","metadata":{}},{"cell_type":"code","source":"#create prompt stype for model\n#define system prompt and include placeholders for question and response generation\n#think and response\n\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\n\"\"\"\n\n#Creating a test medical question for inference\nquestion = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contraction\"\n\n#Enable optimized inference model for Unsloth models\n#which improves speed & efficiency\nFastLanguageModel.for_inference(model)\n\n#Format the question using structured prompt('prompt_style') and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n#generate response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\n\n#decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n#Extract and print only the relevant response part ie after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got the result, but for getting the consise outputs in desired format, we need to finetune with the dataset","metadata":{}},{"cell_type":"markdown","source":"Loading and Processing the dataset FreedomIntelligence/medical-o1-reasoning-SFT from https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT?row=46","metadata":{}},{"cell_type":"code","source":"#Loading first 500 samples from FreedomIntelligence/medical-o1-reasoning-SFT dataset\n#from huggingface hub\nfrom datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[0:500]\",trust_remote_code=True)\ndataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#change the prompt_style for processing the dataset\n\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\n\"\"\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token  #must add EOS_TOKEN\nEOS_TOKEN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#python function to create a \"text\" column in the dataset\n#which consists of train prompt style.\n#fill the placeholders with questions, chains of text and answers\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        #format the inputs, cots and outputs in train_prompt_style format\n        #and append EOS token to mark the end of sentence for each input \n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text) #append the input to the texts list\n    return {\n        \"text\": texts,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#map the text column using formatting_prompts_func function\n\ndataset = dataset.map(formatting_prompts_fuc, batched = True,)\ndataset[\"text\"][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Setting up the model using LoRA","metadata":{}},{"cell_type":"code","source":"#Setup model by adding Low Rank Adapter to the model\n#using peft - Parameter Efficient Fine Tuning, which wraps the base model\n#with LoRA modifications ensurin that only specific parameters are trained\nmodel = FastLangugaeModel.get_peft_model(\n    model,\n    r = 16, #nuomber of trainable parameters for LoRA\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\"\n        \"down_proj\",\n        ],\n        lora_alpha=16,\n        lora_dropout=0,\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=3407,\n        use_rslora=False,\n        loftq_config=None,\n)\n#target_modules specifies the list of modules within the model to be adapted ie in the attention \n#mechanism in the transformer model\n#use_gradient_checkpointing=\"unsloth\" technique to save memory\n#during training \n#loftq - lora finetuning quantization\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#setting training arguments and trainer with model, tokenizer and imp parametrs\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2, #no of processes to use - 2 cpus\n    args=TrainingArguments(\n        per_device_train_batch_size=2, #no of processes per cpu at a time\n        gradient_accumulation_steps=4, #how many steps gradient need to accumulate before updating weights\n        num_train_epochs = 1, #warmup_ratio for full training runs\n        warmup_steps=5,\n        max_steps=60,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10 #loging details every 10 steps\n        optim=\"adamw_8bit\",\n        weight_decay=0.01, #regularisation prevent overfitting\n        lr_scheduler_type=\"linear\", #learning rate\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train the model\ntrainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model inference after finetuning\nquestion = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saving the model locally\nnew_model_local = \"DeepSeek-R1-Medical-COT\"\nmodel.save_pretrained(new_model_local) \ntokenizer.save_pretrained(new_model_local)\n\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}